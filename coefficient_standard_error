#SOURCE:http://people.duke.edu/~rnau/regintro.htm
#LINEAR REGRESSION:
#Linear regression analysis is the most widely used of all statistical techniques: it is the study of linear, additive relationships #between variables.   Let Y denote the “dependent” variable whose values you wish to predict, and let X1, …,Xk denote the “independent” #variables from which you wish to predict it, with the value of variable Xi in period t (or in row t of the data set) denoted by Xit.  #Then the equation for computing the predicted value of Yt is:

#This formula has the property that the prediction for Y is a straight-line function of each of the X variables, holding the others #fixed, and the contributions of different X variables to the predictions are additive.  The slopes of their individual straight-line #relationships with Y are the constants b1, b2, …, bk, the so-called coefficients of the variables.   That is, bi is the change in the #predicted value of Y per unit of change in Xi, other things being equal.  The additional constant b0, the so-called intercept, is the #prediction that the model would make if all the X’s were zero (if that is possible).   The coefficients and intercept are estimated by #least squares, i.e., setting them equal to the unique values that minimize the sum of squared errors within the sample of data to which #the model is fitted. And the model's prediction errors are typically assumed to be independently and identically normally distributed.

#SOURCE:http://stats.stackexchange.com/questions/18208/how-to-interpret-coefficient-standard-errors-in-linear-regression?rq=1
#Parameter estimates, like a sample mean or an OLS regression coefficient, are sample statistics that we use to draw inferences about the #corresponding population parameters. The population parameters are what we really care about, but because we don't have access to the #whole population (usually assumed to be infinite), we must use this approach instead. However, there are certain uncomfortable facts that #come with this approach. For example, if we took another sample, and calculated the statistic to estimate the parameter again, we would #almost certainly find that it differs. Moreover, neither estimate is likely to quite match the true parameter value that we want to know. #In fact, if we did this over and over, continuing to sample and estimate forever, we would find that the relative frequency of the #different estimate values followed a probability distribution. The central limit theorem suggests that this distribution is likely to be #normal. We need a way to quantify the amount of uncertainty in that distribution. That's what the standard error does for you.

#In your example, you want to know the slope of the linear relationship between x1 and y in the population, but you only have access to #your sample. In your sample, that slope is .51, but without knowing how much variability there is in it's corresponding sampling #distribution, it's difficult to know what to make of that number. The standard error, .05 in this case, is the standard deviation of that #sampling distribution. To calculate significance, you divide the estimate by the SE and look up the quotient on a t table. Thus, larger #SEs mean lower significance.

#The residual standard deviation has nothing to do with the sampling distributions of your slopes. It is just the standard deviation of #your sample conditional on your model. There is no contradiction, nor could there be. As for how you have a larger SD with a high R^2 and #only 40 data points, I would guess you have the opposite of range restriction--your x values are spread very widely
